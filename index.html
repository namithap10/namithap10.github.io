<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Namitha Padmanabhan</title>

    <meta name="author" content="Namitha Padmanabhan">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/umd_square.png">
</head>

<body>

    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Namitha Padmanabhan</name>
                                    </p>
                                    <p>I am a second year PhD student in the Department of Computer Science
                                        <!-- <a href="https://www.cs.umd.edu/">Computer Science</a>  -->
                                        at the <a href="https://umd.edu/">University of Maryland</a> (UMD), advised by
                                        Prof <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
                                        where I study computer vision. Prior to this, I completed my master's in Computer
                                        Science from UMD.
                                    </p>
                                    <p>
                                        I received my bachelor's degree in Computer Science from
                                        <a href="https://www.rvce.edu.in/">RVCE</a> Bangalore. During my junior and
                                        senior years,
                                        I had the privilege of working at <a href="https://iisc.ac.in">IISc</a>, with
                                        <a href="https://gtl.csa.iisc.ac.in/hari/">Prof Y Narahari</a>
                                        on multi-armed bandits and <a href="https://ece.iisc.ac.in/~hari/">Prof K V S
                                            Hari</a>.
                                        Before pursuing my master's at UMD, I worked at <a
                                            href="https://www.cisco.com/">Cisco</a>
                                        for 2 years, on web microservices.

                                    </p>
                                    <p>
                                        My current research interests lie in understanding implicit neural
                                        representations
                                        and exploring their utility in computer vision tasks.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:namithap@umd.edu">Email</a> &nbsp/&nbsp
                                        <!-- <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp -->
                                        <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                                        <a href="https://scholar.google.com/citations?user=uwmKc4wAAAAJ&hl=en">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/namitha-padmanabhan/">LinkedIn</a>
                                        <!--<a href="https://twitter.com/">Twitter</a> -->
                                        <!-- <a href="https://github.com/jonbarron/">Github</a> -->
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/np_profile.jpg"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/np_profile.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>


                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/teconerv_teaser.png" alt="teconerv_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>TeCoNeRV: Leveraging Temporal Coherence for
                                        Compressible Neural Representations for Videos</papertitle>
                                    <br>
                                    <strong>Namitha Padmanabhan</strong>,
                                    <a href="https://mgwillia.github.io/">Matthew Gwilliam</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> Preprint. Under review.</em>
                                    <br>
                                    <a href="https://namithap10.github.io/teconerv/">Project Page</a> |<a
                                        href="#">Paper</a> |<a
                                        href="https://github.com/namithap10/TeCoNeRV">Code</a>
                                    <p>
                                        TeCoNeRV is the first hypernetwork-based implicit video compression method that scales to 720p and 1080p, 
                                        while maintaining fast encoding speeds.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/xinc_teaser.png" alt="xinc_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Explaining the Implicit Neural Canvas (XINC): Connecting Pixels to
                                        Neurons by Tracing their Contributions</papertitle>
                                    <br>
                                    <strong>Namitha Padmanabhan*</strong>,
                                    <a href="https://mgwillia.github.io/">Matthew Gwilliam*</a>,
                                    <a href="https://www.cs.umd.edu/~pulkit/">Pulkit Kumar</a>,
                                    <a href="https://www.cs.umd.edu/~shishira/">Shishira R Maiya</a>,
                                    <a href="https://maxlikelihood.ai/">Max Ehrlich</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>,
                                    2024
                                    <br>
                                    <a href="https://namithap10.github.io/xinc/">Project Page</a> |<a
                                        href="https://arxiv.org/abs/2401.10217">Paper</a> |<a
                                        href="https://github.com/namithap10/xinc">Code</a>
                                    <p>
                                        XINC dissects Implicit Neural Representation (INR) models to understand how
                                        neurons
                                        represent images and videos and to reveal the inner workings of INRs.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/tats_framework.png" alt="tats_framework" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Trajectory-aligned Space-time Tokens for Few-shot Action Recognition
                                    </papertitle>
                                    <br>
                                    <a href="https://www.cs.umd.edu/~pulkit/">Pulkit Kumar</a>,
                                    <strong>Namitha Padmanabhan</strong>,
                                    <span>Luke Luo</sup></span>,
                                    <a href="https://rssaketh.github.io/">Saketh Rambhatla</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> Proceedings of the European Conference on Computer Vision (ECCV)</em>, 2024
                                    <br>
                                    <a href="https://www.cs.umd.edu/~pulkit/tats/">Project Page</a> |<a
                                        href="https://arxiv.org/abs/2407.18249">Paper</a>
                                    <p>Few-shot action recognition with disentanglement of motion and appearance
                                        representations
                                        by harnessing point trackers and self-supervised representations.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/diffssl_teaser.png" alt="diff_ssl" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Do Text-free Diffusion Models Learn Discriminative Visual
                                        Representations?</papertitle>
                                    <br>
                                    <a href="https://mgwillia.github.io/">Matthew Gwilliam*</a>,
                                    <a href="https://soumik-kanad.github.io/">Soumik Mukhopadhyay*</a>,
                                    <span>Yosuke Yamaguchi<sup>&#10013;</sup></span>,
                                    <a href="https://scholar.google.com/citations?user=DJRhPVgAAAAJ&hl=en">Vatsal
                                        Agarwal<sup>&#10013;</sup></a>,
                                    <strong>Namitha Padmanabhan</strong>,
                                    <a href="https://archana1998.github.io/">Archana Swaminathan</a>,
                                    <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> Proceedings of the European Conference on Computer Vision (ECCV)</em>, 2024
                                    <br>
                                    <a href="https://mgwillia.github.io/diffssl/">Project Page</a> |<a
                                        href="https://arxiv.org/abs/2311.17921">Paper</a>
                                    <p>Exploring diffusion models as unified unsupervised image representation learning
                                        models for many recognition tasks. Proposed DifFormer and DifFeed, novel
                                        mechanisms for fusing diffusion features
                                        for image classification.</p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/metabit_method.png" alt="metabit" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed
                                        Video Quality Enhancement</papertitle>
                                    <br>
                                    <a href="https://maxlikelihood.ai/">Max Ehrlich</a>,
                                    <a href="https://scholar.google.com/citations?user=AU0-C00AAAAJ&hl=en">Jon
                                        Barker</a>,
                                    <strong>Namitha Padmanabhan</strong>,
                                    <a href="https://scholar.google.com/citations?user=lc0ARagAAAAJ&hl=en">Larry S
                                        Davis</a>,
                                    <a href="https://scholar.google.com/citations?user=Wel9l1wAAAAJ&hl=en">Andrew
                                        Tao</a>,
                                    <a href="https://scholar.google.com/citations?user=UZ6kI2AAAAAJ&hl=en">Bryan
                                        Catanzaro</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2024
                                    <br>
                                    <a href="https://arxiv.org/abs/2202.00011">Paper</a>
                                    <p>
                                        Restore detail in compressed videos by leveraging structure and motion
                                        information
                                        from the video bitstream, handling various compression quality settings.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/preprint_diffssl_teaser.png" alt="preprint_diff_ssl" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Diffusion Models Beat GANs on Image Classification</papertitle>
                                    <br>
                                    <a href="https://mgwillia.github.io/">Matthew Gwilliam*</a>,
                                    <a href="https://soumik-kanad.github.io/">Soumik Mukhopadhyay*</a>,
                                    <a href="https://scholar.google.com/citations?user=DJRhPVgAAAAJ&hl=en">Vatsal
                                        Agarwal</a>,
                                    <strong>Namitha Padmanabhan</strong>,
                                    <a href="https://archana1998.github.io/">Archana Swaminathan</a>,
                                    <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> preprint only </em>
                                    <br>
                                    <a href="https://mgwillia.github.io/diff-ssl/">Project Page</a> |<a
                                        href="https://arxiv.org/abs/2307.08702">Paper</a>
                                    <p>Show the potential of diffusion models as unified unsupervised image
                                        representation learners.</p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>


                                <!-- <td> -->


                                <!-- <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, CVPR 2021</a>
              <br><br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br><br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        <a href="https://jonbarron.info/">Template credits</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>

</body>

</html>